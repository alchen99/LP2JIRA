{
    "total_size": 107,
    "start": 0,
    "next_collection_link": "https://api.launchpad.net/devel/trafodion/all_specifications?ws.size=75&memo=75&ws.start=75",
    "entries": [
        {
            "starter_link": "https://api.launchpad.net/devel/~steve-varnau",
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-upgrade-puppet/linked_branches",
            "lifecycle_status": "Complete",
            "title": "infra - Update to puppet 3.x",
            "definition_status": "New",
            "milestone_link": null,
            "priority": "High",
            "http_etag": "\"18961091a1cbff34381482b5277c4e142e631e3e-7e9ec19d1f6ea76724d93c56834b03e9bd5e33c9\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-upgrade-puppet",
            "information_type": "Public",
            "date_started": "2015-03-11T21:15:17.547243+00:00",
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": "https://api.launchpad.net/devel/~steve-varnau",
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-upgrade-puppet/bugs",
            "is_started": true,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-upgrade-puppet/dependencies",
            "specification_url": null,
            "assignee_link": "https://api.launchpad.net/devel/~steve-varnau",
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": false,
            "workitems_text": "Work items:\nTest set-up of 14.x server with puppet3: DONE\nTest puppetmaster node serving to itself: DONE\nTest puppetmaster node serving to slave with puppet2: DONE\nTest puppetmaster node serving to slave with puppet3: DONE\nSetup puppet3 server: DONE\nUpdate Base linux image (for new slaves): DONE\nRoll existing AHW and CM slaves to new server and update puppet client: DONE\nRoll existing servers one by one to new server and update puppet client: INPROGRESS",
            "date_completed": "2015-03-11T21:15:17.547243+00:00",
            "name": "infra-upgrade-puppet",
            "whiteboard": "Ubuntu 14.04 requires puppet 3.x\nPuppetmaster needs to be upgraded first.  It should be backwards compatible with 2.x clients. This needs to be verified however. Upgrading the Puppetmaster will also require upgrading the OS of the puppetmaster server to Ubuntu 14.04 LTS",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/infra-upgrade-puppet",
            "summary": "Puppet 2.7 is end of life as of October 2014",
            "owner_link": "https://api.launchpad.net/devel/~steve-varnau",
            "approver_link": null,
            "drafter_link": "https://api.launchpad.net/devel/~steve-varnau",
            "date_created": "2014-09-23T23:37:41.578332+00:00",
            "is_complete": true,
            "implementation_status": "Implemented"
        },
        {
            "starter_link": "https://api.launchpad.net/devel/~suresh-subbiah",
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/bulk-loader/linked_branches",
            "lifecycle_status": "Complete",
            "title": "Bulk Loader for Trafodion",
            "definition_status": "Approved",
            "milestone_link": null,
            "priority": "High",
            "http_etag": "\"857083ba50e0ab4c7c5f0f40442d5adebb8123a3-4a3cafc67a42cdf60d1f87d9f0dbbfe6c2047026\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/bulk-loader",
            "information_type": "Public",
            "date_started": "2014-11-19T01:22:02.809488+00:00",
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": "https://api.launchpad.net/devel/~suresh-subbiah",
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/bulk-loader/bugs",
            "is_started": true,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/bulk-loader/dependencies",
            "specification_url": null,
            "assignee_link": "https://api.launchpad.net/devel/~khaled-bouaziz",
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": true,
            "workitems_text": "",
            "date_completed": "2014-11-19T01:22:02.809488+00:00",
            "name": "bulk-loader",
            "whiteboard": "1.\tWhat is Trafodion Bulk Load\nTrafodion Bulk Load is the process of preparing and loading Hfiles directly in the region servers and bypassing the write path and the cost associated with it.  The write path begins at a client, moves to a region server, and ends when data eventually is written to an HBase data file called an HFile.\nThe Trafodion bulk load process takes place in two phases:\n1.\t Preparation phase\no\t In This step Trafodion reads the data from the source files in hive/hdfs, partitions the data based on the target table partitioning scheme, sorts the data and then generates KeyValue pairs that will populate the Hfiles. Trafodion also encode the data for faster storage and retrieval.\n2.\t Loading the files into Hbase phase\no\tThis step uses the LoadIncrementalHFiles (aka computebulkload tool) and load the generated HFiles into the region servers.\n2.\tSyntax\nThe Trafodion Bulk load syntax is described below\n\n    LOAD   [with option[,option,….]] INTO <target-table> SELECT .. FROM <source-table>\n\nWhere:\n•\t<target-table> is the target Trafodion table where the data will be loaded.\n•\t<source-table> can be a Trafodion table or a hive table that has the source data. Hive tables can be accessed in Trafodion using the hive.hive schema (example hive.hive.orders). The hive table needs to already exist in hive before Trafodion can access it.  If user wants to load data that is already in an hdfs folder then they need to create an external hive table with the right fields and pointing the hdfs folder containing the data. Users can also specify a WHERE clause on the source data as a filter.\n•\t[With option[,option,…]] is a set of options that user can specify and they can be zero or more of the following:\no\tTRUNCATE TABLE: By default target table is not truncated before loading data.  If truncate table option is specified the target table is truncated before starting the load.\no\tNO RECOVERY: by default Trafodion Bulk Load handles recovery using hbase snapshots mechanism.   If no recovery option is specified then snapshots are not used.\no\tNO POPULATE INDEXES: by default index maintenance are handled by Trafodion Bulk Load and indexes are disabled before starting the load and populated after the load is complete.   If no populate indexes option is specified, indexes are not handled by load.\no\tNO DUPLICATE CHECK: This options check if there are duplicates coming from the source only. By default an error is generated when duplicates from the source data are detected. If no Duplicate check option is specified then duplicates are ignored.\no\tNO OUTPUT: by default load command print status messages listing the steps that are being executed. If no output is specified then no status messages are displayed.\n•\tExample: below is an example of running the load statement to load customer_demographics_salt table from hive table hive.hive.customer>\n         >>load with populate indexes into customer_demographics_salt\n+>select * from hive.hive.customer_demographics where cd_demo_sk <= 5000;\nTask: LOAD             Status: Started    Object: TRAFODION.HBASE.CUSTOMER_DEMOGRAPHICS_SALT\nTask:  DISABLE INDEXE  Status: Started    Object: TRAFODION.HBASE.CUSTOMER_DEMOGRAPHICS_SALT\nTask:  DISABLE INDEXE  Status: Ended      Object: TRAFODION.HBASE.CUSTOMER_DEMOGRAPHICS_SALT\nTask:  PREPARATION     Status: Started    Object: TRAFODION.HBASE.CUSTOMER_DEMOGRAPHICS_SALT\n       Rows Processed: 5000\nTask:  PREPARATION     Status: Ended      ET: 00:00:03.199\nTask:  COMPLETION      Status: Started    Object: TRAFODION.HBASE.CUSTOMER_DEMOGRAPHICS_SALT\nTask:  COMPLETION      Status: Ended      ET: 00:00:00.331\nTask:  POPULATE INDEX  Status: Started    Object: TRAFODION.HBASE.CUSTOMER_DEMOGRAPHICS_SALT\nTask:  POPULATE INDEX  Status: Ended      ET: 00:00:05.262\n\n3.\tConfiguration\n3.1.\tStaging folder for Hfiles\nTrafodion Bulk Load uses an HDFS folder as staging area for the hfiles prior to calling hbase APIs to merge them into the Trafodion table.  By default Trafdion uses “/bulkload/” as the staging folder. This folder needs to be owned by the same user as the one under which Trafodion runs. Trafodion also need to have full permissions on this folder. The hbase user (user under which hbase runs)\nneeds to have read/write access to this folder.\nExample :    drwxr-xr-x   - trafodion trafodion           0 2014-07-07 09:49 /bulkload.\n3.2 Access Control Lists (ACL)\nAs of trafodion 0.9 users can also use ACLs to set the right permissions on the /bulkload folder.  \nmore information on ACLs can be found at: http://hadoop.apache.org/docs/r0.18.3/hdfs_permissions_guide.html\n\n\n2.\tHDFS\nTo load data stored in hdfs, we need to create a hive table with the right fields and types pointing to hdfs folder containing the data before we start the load. The data files under the hdfs folder need to be in delimited format. the Row delimiter is the hive row delimiter which is '\\n' (Ascii 0x10). The field delimiter nees also to be one byte in  length with values between 1 and 255  (decimal). THe data in the data files needs to be either in ISO-8851or UTF-8 charecter set.  Currently character sets other than ISO-88591 and UTF-8 are not supported.\n\n3.\tSnapshots\nIf the NO RECOVERY OPTION is not specified, Trafodion Bulk Load uses hbase snapshots as mechanism for recovery. Snapshots are light weight operation where some metadata is copied (data is not copied). A snapshot is taken before the load starts and removed after the load completes successfully. If something goes wrong and it is possible to recover then the snapshot is used to restore the table to its initial state before the load started. Hbase also needs to be configured to allow snapshots.\n\n4.\tExamples:\n4.1. Example 1:\nFor customer demographics data residing in /hive/tpcds/customer_demographics, we create an external hive table using the below hive SQL\ncreate external table customer_demographics\n(\n    cd_demo_sk                int,\n    cd_gender                 string,\n    cd_marital_status         string,\n    cd_education_status       string,\n    cd_purchase_estimate      int,\n    cd_credit_rating          string,\n    cd_dep_count              int,\n    cd_dep_employed_count     int,\n    cd_dep_college_count      int\n)\nrow format delimited fields terminated by '|'\nlocation '/hive/tpcds/customer_demographics';\n\nThe Trafodion table where we want t o load the data is defined using the DDL:\n\ncreate table customer_demographics_salt\n(\n  cd_demo_sk              int not null,\n  cd_gender               char(1),\n  cd_marital_status       char(1),\n  cd_education_status     char(20),\n  cd_purchase_estimate    int,\n  cd_credit_rating        char(10),\n  cd_dep_count            int,\n  cd_dep_employed_count   int,\n  cd_dep_college_count    int,\n  primary key (cd_demo_sk)\n)\nsalt using 4 partitions on (cd_demo_sk);\nTo load the data we can use the statement:\nload with populate indexes into customer_demographics_salt select * from hive.hive.customer_demographics where cd_demo_sk <= 5000;\n\n4.4. Example 2:\n\n\n4.4.1.Mapping HDFS data to a Hive table \nTo load data stored in hdfs, a hive table with right fields and types need to be created and pointed to hdfs folder containing data.  The hive file must be of format \nTEXTFILE\n\nFor example, if a table contains data with 2 fields(string and int) delimited by ‘|’ in folder ‘/hive/tpcds/customer_demographics’, then the following create statement \nneed to be run in hive shell:\ncreate external table customer_demographics\n(\n    cd_demo_sk                int,\n    cd_name                 string\n)\nrow format delimited fields terminated by '|' \nlocation '/hive/tpcds/customer_demographics';\n\n4.4.2.Staging folder for Hfiles\nTrafodion Bulk Load uses an HDFS folder as staging area for temporary hfiles prior to calling hbase APIs to merge them into target Trafodion table.  By default Trafdion uses “/bulkload/” as the staging folder. This folder is created by the installer with appropriate permissions. On some systems there may be access permission errors related to this folder. If such an error is seen please contact Trafodion development. \n\n\n4.4.3.Trafodion Table\nA Trafodion table need to be created where data will be loaded.\n\nFor example, table corresponding to hdfs data from the previous section will be:\ncreate table Trafodion.sch.customer_demographics\n(\ndemo_sk int not null,\nname varchar(100),\nprimary key (demo_sk)\n)\nhbase_options\n  (\n    DATA_BLOCK_ENCODING = 'FAST_DIFF',\n    COMPRESSION = 'SNAPPY'\n  )\nsalt using 8 partitions on (demo_sk) ;\n\nOther compression like ‘GZ’ could also be used. For salting some planning should be done to determine how many partitions a table should have and what the partitioning keys should be.\n4.4.4.Loading data\nOnce both hive and Trafodion tables have been created, data can be loaded using the following command:\n\nload with populate indexes into trafodion.sch.customer_demographics \n           select * from hive.hive.customer_demographics\n\nThis will load data into Trafodion table and populate all indexes that are created on that table. If tables have indexes and disk space is a concern, each index may need to have data_block_encoding and compression enabled. For index this cannot be done in the CREATE INDEX statement, please contact Trafodion development on how these options can be enabled for an index.\n\n",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/bulk-loader",
            "summary": "HBase has a feature to produce HFiles from bulk data and to integrate them into an existing table. Khaled has designed and implemented a bulk loader that is based on this feature.\n\n                                                  Bulk loader for Trafodion\n\n1.       What is HBase bulk load:\nHBase bulk load is the process of preparing and loading Hfiles which are HBase own format directly in the region servers and bypassing the write path and the cost associated with it.  The write path begins at a client, moves to a region server, and ends when data eventually is written to an HBase data file called an HFile. \n2.       The HBase bulk load process takes place in two phases:\n2.1.    Preparation phase\n2.1.1.  This step is usually done as a map reduce job that extracts the data from the source files and generated KeyValue pairs that will populate the Hfiles. The keyvalues pairs need to be sorted and the map reduce job should be configured to do the sorting on the generated keyvalues \n2.1.2. In the case of trafodion one more thing needs to done: data needs to be encoded \n2.2.    Loading the files into Hbase phase\n2.2.1. This step uses the LoadIncrementalHFiles (aka computebulkload tool) and load the generated HFiles into the region servers. This step should not take a lot of time nor resources\n3.       Integration of Trafodion and Hbase Bulk load.\nTwo approaches were studied \n3.1.    Approach 1: in this approach, the data preparation is done using Trafodion. Trafodion reads the data from source files in HDFS (hive), does the sort and converts and encodes the data using Trafodion expressions and  then produces keyvalue pairs and write them to Hfiles on the hadoop file system directly bypassing the hbase client and the write path.   The sort is also done In Trafodion.\n3.2.    Approach 2 : In this approach, data preparation is done using a custom mapreduce jobs. In this case we may need to replicate the encoding/expressions mechanism that is used in trafodion.  It also requires querying the metadata and figure out all the conversions.\n3.3.    \n4.       Approach selected  \n4.1.    For now I started implementing Approach 1 and have a working prototype. For approcah2, we can implement it later if we need to.\nTo load data the user can use the load command \n                   load into trafodion.sch.orders  select o_orderkey, o_custkey, o_orderstatus, o_totalprice, cast(o_orderdate as date),        \n                   o_orderpriority, o_clerk, o_shippriority, o_comment from hive.hive.orders;   \nand the explain plan in Trafodion is shown below:\n5.       What is supported and not supported for the first delivery  after approval\n5.1.    The preparation phase can generate Hfiles in parallel\n5.2.    Indexes and check constarints are not supported in the first deliverable and will be supported in future deliverables\n5.3.    Transactions are not supported and if the table is not empty and something goes wrong it may not be recoverable. A check if the table is empty or not can be added\n5.4.    The Hfiles that are produced by the preparation phase are stored in HDFS and may take as much space as the original source files. Once the loading phase is done the Hfiles are removed\n5.5.    Exceptions and error rows logging is not supported in this deliverable and will be supported in future deliverables\n5.6.    I implemented CIF for the move expression in the HDFSScan\n5.7.    Performance optimizations  will be done in future deliveries and may include changes to the plan and other optimizations\n",
            "owner_link": "https://api.launchpad.net/devel/~hans-zeller",
            "approver_link": null,
            "drafter_link": "https://api.launchpad.net/devel/~khaled-bouaziz",
            "date_created": "2014-06-26T23:25:36.596540+00:00",
            "is_complete": true,
            "implementation_status": "Implemented"
        },
        {
            "starter_link": null,
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-winodbc/linked_branches",
            "lifecycle_status": "Not started",
            "title": "infra - Automate build of win-odbc64 driver",
            "definition_status": "Approved",
            "milestone_link": null,
            "priority": "High",
            "http_etag": "\"669c2063ccda3fbee371c3db9101251dd0bec1f2-4f15bf9bf76495ca15c11c2d2b30aaf871b4f48b\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-winodbc",
            "information_type": "Public",
            "date_started": null,
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": null,
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-winodbc/bugs",
            "is_started": false,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-winodbc/dependencies",
            "specification_url": null,
            "assignee_link": null,
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": false,
            "workitems_text": "Work items:\nSet up Windows VM : TODO\nClean up build process so version only needs to be changed in 1 file : TODO",
            "date_completed": null,
            "name": "infra-winodbc",
            "whiteboard": "Required Software:\nopenssl >= 0.9.8\nMicrosoft Visual Studio >= 2010\n64bit odbccp32.lib from Microsoft Visual Studio 2008 or previous.\nInstallShield >= 2012\ngit > 1.9",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/infra-winodbc",
            "summary": "Need scripted automation\nNeed windows VM as a jenkins slave, with required software\nFollow-on work (dependent on this) will be to add smoke testing.",
            "owner_link": "https://api.launchpad.net/devel/~steve-varnau",
            "approver_link": null,
            "drafter_link": "https://api.launchpad.net/devel/~steve-varnau",
            "date_created": "2014-07-06T21:39:55.489170+00:00",
            "is_complete": false,
            "implementation_status": "Unknown"
        },
        {
            "starter_link": "https://api.launchpad.net/devel/~steve-varnau",
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-horton1.3/linked_branches",
            "lifecycle_status": "Complete",
            "title": "infra - HortonWorks 1.3 build test nodes",
            "definition_status": "Superseded",
            "milestone_link": null,
            "priority": "Essential",
            "http_etag": "\"355dbc40aab65f197a3db0d873c679ecdf9a1f02-8f15789983ec3a8ab8b7b6355549ab4505a7fbbe\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-horton1.3",
            "information_type": "Public",
            "date_started": "2014-08-09T00:28:27.853443+00:00",
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": "https://api.launchpad.net/devel/~steve-varnau",
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-horton1.3/bugs",
            "is_started": true,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-horton1.3/dependencies",
            "specification_url": null,
            "assignee_link": "https://api.launchpad.net/devel/~steve-varnau",
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": false,
            "workitems_text": "Work items:\nJenkins Jobs include Distro (Stacey): DONE\nZuul Jobs use Distro Jobs (Stacey): INPROGRESS\nPuppet node definition: DONE\nJenkins node-test: INPROGRESS\nBring up slaves: TODO\nAdd jenkins job definitions: TODO\nAdd jobs to zuul (non-voting): TODO\nMake jobs voting: TODO",
            "date_completed": "2014-09-05T20:48:42.966633+00:00",
            "name": "infra-horton1.3",
            "whiteboard": "Briefly blocked due to Trafodion code incompatibility with Hadoop 1. Hans has delivered a fix and moving forward again. - 8/12/14\nNow blocked on java compile error. - 8/12/14\n\nThis will be superseded by HDP2.1 distro. Trafodion will soon require HBase 0.98. - Abandoning 1.3 as a build/test environment.",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/infra-horton1.3",
            "summary": "Add hortonworks nodes for automated build & test\n\nNote: This requires some code changes, since Hortonworks HDP 1.3 uses Hadoop 1.0, and one libhdfs call (hdfsDelete) changed slightly. See https://review.trafodion.org/#/c/232/.\n\nAlthough the parameters of hdfsDelete are different, when we tested Trafodion objects that used\nthe Hadoop 1.0 parameters and a Hadoop instance that uses Hadoop 2.0, the code still executed\nsuccessfully, probably because the extra parameter in 2.0 is not needed by our code.",
            "owner_link": "https://api.launchpad.net/devel/~steve-varnau",
            "approver_link": null,
            "drafter_link": "https://api.launchpad.net/devel/~steve-varnau",
            "date_created": "2014-07-06T21:34:32.504091+00:00",
            "is_complete": true,
            "implementation_status": "Blocked"
        },
        {
            "starter_link": "https://api.launchpad.net/devel/~roberta-marton",
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/instrument-secure-hadoop/linked_branches",
            "lifecycle_status": "Started",
            "title": "Instrument Trafodion to work with Secure Hadoop",
            "definition_status": "Drafting",
            "milestone_link": "https://api.launchpad.net/devel/trafodion/+milestone/r1.1",
            "priority": "High",
            "http_etag": "\"2276893b9ce1fa02192fee83d26c9553f76569a2-8628d27c98ee1daa0ada61e6ea08e51e167d3473\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/instrument-secure-hadoop",
            "information_type": "Public",
            "date_started": "2015-02-06T18:11:08.779244+00:00",
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": null,
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/instrument-secure-hadoop/bugs",
            "is_started": true,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/instrument-secure-hadoop/dependencies",
            "specification_url": "http://docs.trafodion.org/Secure_hadoop.pdf",
            "assignee_link": "https://api.launchpad.net/devel/~roberta-marton",
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": false,
            "workitems_text": "",
            "date_completed": null,
            "name": "instrument-secure-hadoop",
            "whiteboard": "First phase for incorporating Secure Hadoop into Trafodion has started.",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/instrument-secure-hadoop",
            "summary": "The next step to enhance Trafodion security is to seamlessly integrate within theSecure  Hadoop eco-system.\n \nTrafodion is installed on top of the Hadoop and supports authentication through OpenLDAP and authorization through Trafodion; however, Hadoop, by itself runs in a non-secure mode. This blueprint defines a task to configure Trafodion to run in with Secure Hadoop.  When the secure mode is instrumented, each user and service will be authenticated by Kerberos which include all products Trafodion uses in its eco-system. The means that  secure versions of Hadoop, HBase, Zookeeper, and others will be integrated.\n",
            "owner_link": "https://api.launchpad.net/devel/~roberta-marton",
            "approver_link": null,
            "drafter_link": "https://api.launchpad.net/devel/~roberta-marton",
            "date_created": "2014-10-31T17:54:13.728574+00:00",
            "is_complete": false,
            "implementation_status": "Started"
        },
        {
            "starter_link": "https://api.launchpad.net/devel/~suresh-subbiah",
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/bulkunload/linked_branches",
            "lifecycle_status": "Started",
            "title": "Bulk Unload",
            "definition_status": "Approved",
            "milestone_link": null,
            "priority": "Medium",
            "http_etag": "\"16df402b2ddc311f52e95397357092e9cbf0805a-77508673c3b4ecc019f561b8594fe25dc3907f39\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/bulkunload",
            "information_type": "Public",
            "date_started": "2014-11-19T02:23:45.421099+00:00",
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": null,
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/bulkunload/bugs",
            "is_started": true,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/bulkunload/dependencies",
            "specification_url": null,
            "assignee_link": "https://api.launchpad.net/devel/~khaled-bouaziz",
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": true,
            "workitems_text": "",
            "date_completed": null,
            "name": "bulkunload",
            "whiteboard": "Unload commands can be issued from any existing SQL client such as sqlci or TRAFCI.  Data will be extracted to a HDFS location.\n\nUNLOAD [WITH <options-list>]   INTO <target-location> <query>;\n\n<target-location> : HDFS folder to which extracted data is written. The name of folder must be enclosed in single quotes. The folder name must be given as a full pathname and not as a relative path. The Trafodion user should have write permissions on this folder. If run in parallel multiple files will be produced under <target-location>. The number of files created will equal the number of ESPs.\n<options-list> : Unload options. The WITH clause is optional. If an option is specified more than once, an error with SQLCODE -4489 will be raised.\n<options-list> : <option>  <options-list>\n<option> : DELIMITER ‘<delimiter-literal>’ | <delimiter-ascii-value>\n                    RECORD_SEPARATOR ‘<separator-literal>’ | <separator-ascii-value>\n                    NULL_STRING ‘<string-literal>’\n                    PURGEDATA FROM TARGET\n                    COMPRESSION GZIP\n                    MERGE FILE <merged_file-path> [OVERWRITE]\n                    NO OUTPUT\n\n- The DELIMITER option can be used to specify one character that separates consecutive fields in the same row. If this option is not specified the character '|' will be used. The delimiter can also be specified as an ASCII value. Valid values range from 1 to 255. The value must be specified in decimal notation. In later releases we will support specifying the delimiter value in hexadecimal or octal notation. When the ASCII value is specified, the delimiter can only be one character wide. No quotes should be used when the delimiter is specified through an ASCII value.  Escape sequences such as '\\a', '\\b', '\\f', '\\n', '\\r', '\\t', or '\\v' are also allowed.\n- The RECORD_SEPARATOR option can be used to specify the character that will be used to separate consecutive records or rows in the output file. The default value is a newline character. The record_separator can also be specified as an ascii value. Valid values range from 1 to 255. The value must be specified in decimal notation. In later releases we will support specifying the record_separator value in hexadecimal or octal notation. When the ASCII value is specified, the record_separator can only be one character wide. No quotes should be used when the record_separator is specified through an ASCII value. Escape sequences such as '\\a', '\\b', '\\f', '\\n', '\\r', '\\t', or '\\v' are also allowed.\n- The NULL_STRING option can be used to specify the string that will be used to indicate a NULL value. The default value is the empty string ''.\n- PURGEDATA FROM TARGET  When this option specified the files under the  <hdfs-folder> folder are deleted\n-  COMPRESSION GZIP: when this option is specified the Gzip compression is    used. the compression takes place in the extract node and data  is written to disk in compressed format (Gzip i the only supported        compression for now)\n- MERGE FILE <merged-file-path> [OVERWRITE]: When this option is specified the files  unloaded are merged into one single file <merged-file-path>. if   compression is specified the data unloaded in compressed format  and the merged file will be in compressed format also. It he optional OVERWRITE keyword is specified the file is overwritten other an error is raised if the file already exists\n- NO OUTPUT: If this option is specified then no status message is displayed\n- <query>:a query that can be a simple query or complex one that contains GROUP BY, JOIN or UNION clauses. The ORDER BY Clause is not currently supported and using will produce a syntax error.\n\nExample\n>>UNLOAD\n+>WITH PURGEDATA FROM TARGET\n+>MERGE FILE  'merged_customer_demogs.gz' OVERWRITE\n+>COMPRESSION GZIP\n+>INTO '/bulkload/customer_demographics'\n+>select * from trafodion.hbase.customer_demographics\n+><<+ cardinality 10e10 >>;\nTask: UNLOAD           Status: Started\nTask:  EMPTY TARGET    Status: Started\nTask:  EMPTY TARGET    Status: Ended      ET: 00:00:00.014\nTask:  EXTRACT         Status: Started\n       Rows Processed: 200000\nTask:  EXTRACT         Status: Ended      ET: 00:00:04.743\nTask:  MERGE FILES     Status: Started\nTask:  MERGE FILES     Status: Ended      ET: 00:00:00.063\n\n--- 200000 row(s) unloaded.",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/bulkunload",
            "summary": "Bulk Unload is a tool to unload data from trafodion tables into an hdfs location specified by the user. Extracted data can be either compressed or uncompressed based on what the user chooses.",
            "owner_link": "https://api.launchpad.net/devel/~khaled-bouaziz",
            "approver_link": "https://api.launchpad.net/devel/~suresh-subbiah",
            "drafter_link": "https://api.launchpad.net/devel/~khaled-bouaziz",
            "date_created": "2014-10-13T20:24:18.981514+00:00",
            "is_complete": false,
            "implementation_status": "Beta Available"
        },
        {
            "starter_link": "https://api.launchpad.net/devel/~hans-zeller",
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/cmp-tmudf-compile-time-interface/linked_branches",
            "lifecycle_status": "Started",
            "title": "Complete work for TMUDF compile time interface",
            "definition_status": "Approved",
            "milestone_link": "https://api.launchpad.net/devel/trafodion/+milestone/r1.1",
            "priority": "Medium",
            "http_etag": "\"ffa6d7389d367aa1ca76a60ee9dc2a6d41ca85a8-95df17df96ce8efac8feb8652f68378147066254\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/cmp-tmudf-compile-time-interface",
            "information_type": "Public",
            "date_started": "2014-11-19T01:22:19.752222+00:00",
            "has_accepted_goal": true,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": null,
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/cmp-tmudf-compile-time-interface/bugs",
            "is_started": true,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/cmp-tmudf-compile-time-interface/dependencies",
            "specification_url": "https://wiki.trafodion.org/wiki/index.php/Tutorial:_The_object-oriented_UDF_interface",
            "assignee_link": "https://api.launchpad.net/devel/~hans-zeller",
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": true,
            "workitems_text": "",
            "date_completed": null,
            "name": "cmp-tmudf-compile-time-interface",
            "whiteboard": "Gerrit topic: https://review.trafodion.org/#q,topic:bp/cmp-tmudf-compile-time-interface,n,z\n\n\nAddressed by: https://review.trafodion.org/787\n    TMUDF C++ compiler interface, part of log-reading TMUDF\n\n\nAddressed by: https://review.trafodion.org/824\n    Phase 2 for log reader TMUDF\n\n\nAddressed by: https://review.trafodion.org/848\n    Log reading TMUDF, phase 3\n\n\nGerrit topic: https://review.trafodion.org/#q,topic:bug/1420539,n,z\n\n\nAddressed by: https://review.trafodion.org/1125\n    C++ run-time interface for TMUDFs\n\n\nGerrit topic: https://review.trafodion.org/#q,topic:bug/cmp-tmudf-compile-time-interface,n,z\n\n\nAddressed by: https://review.trafodion.org/1249\n    Normalizer interface for TMUDFs, blueprint cmp-tmudf-compile-time-interface\n\n\nAddressed by: https://review.trafodion.org/1279\n    Normalizer interface for TMUDFs, blueprint cmp-tmudf-compile-time-interface\n",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/cmp-tmudf-compile-time-interface",
            "summary": "Suresh and others implemented much of a compile time interface for TMUDFs (Table-Mapping UDFs). Such an interface allows a TMUDF to be polymorphic (input and output columns decided at compile time, not at create time) and to do optimizations like elimination of unneeded columns, pushing predicates below or into the TMUDF, getting better cardinality and cost estimates, and using sort order and partitioning of input tables.\n\nA TMUDF can have zero or more input tables. Using more than one input table is not tested and\nsupported at the moment, but the design should allow it.\n\nThe interface will be a C++ class that the TMUDF writer can derive from. Implementing a compiler\ninterface for a TMUDF is completely optional and is done by overriding virtual methods of the\ndefault implementation. In a later step, we also want to replace the C interface\nat runtime with this C++ interface, designed a few years ago. We should be able to define a\nJava compile time interface that's fairly similar to the one in C++. A TMUDF writer only needs to\noverride those interfaces that need to be different from the default implementation. For example,\na TMUDF could define its output columns through the compiler interface, but it might not\nsupport pushing predicates into the TMUDF. Another example could be a TMUDF that only\nimplements the compiler interface that determines the degree of parallelism.\n\nThere are three main classes in this interface, all are defined in file core/sql/sqludr/sqludr.h:\n\nUDRInvocationInfo: This is similar to SQLUDR_TMUDFINFO in the C interface. It describes\nthe metadata of the TMUDF, scalar input parameters, the table-valued result, PARTITION BY\nand ORDER BY clauses specified for table-valued inputs, etc. There is one of these for every\nTMUDF invocation in a query. In some cases, the compiler may create additional\nUDRInvocationInfo objects when it transforms the TMUDF, for example by placing it under\na nested join, with a different set of predicates to be pushed down. There are additional\nclasses to describe parameters, table-valued inputs and outputs, data types, similar to\nthe existing C interface.\n\nUDRPlanInfo: There are zero or more UDRPlanInfo objects for every UDRInvocationInfo\nobject. The optimizer creates one for every optimization goal (context) where it needs to\ncall the TMUDF interface.\n\nTMUDRInterface: This class represents the code associated with a TMUDF. The class itself\nrepresents the default behavior of a TMUDF without a compile time interface. UDF writers\ncan define a derived class and implement virtual methods to customize the optimizer\ninterface. Trafodion tries to find a C function\n<UDF external function name>_CreateCompilerInterfaceObject\nIf that function exists in the UDF library, it is assumed to return an object of a class\nderived from TMUDRInterface, and the compiler will call the virtual methods, some\ncould be defined in the derived class, some could be in the base class, and the derived\nclass also might call the base class method to do part of the work.\n\nHere are the methods we plan to support:\n\n- Validate scalar input parameters, possibly allow those parameters to deviate from the\n  parameter list declared at DDL time.\n- Allow the compiler interface to look at constant values that are passed in as input\n  parameters.\n- Define the table-valued result columns, based on scalar parameters and column\n  layout of the input (child) table(s).\n- Eliminate unneeded columns from the TMUDF result and also from the input tables.\n- Allow predicates to be pushed down through he TMUDF operator to the child table(s).\n- Allow predicates to be absorbed into the TMUDF.\n- Return a cost estimate of the TMUDF, based on information available at compile time.\n- Influence the degree of parallelism chosen for the TMUDF.\n- Make use of natural partitioning and sort order of input (child) tables to produce\n  partitioned and sorted results.\n- Gather the necessary information based on the compile time interaction that is\n  needed at runtime.\n- At any time in the process, the compile time interface can raise an exception. If\n  it does, the compilation will fail and an error message provided by the TMUDF\n  writer will be returned in the diagnostics area.\n\nSome more design choices:\n\nThe C++ interface uses its own C++ namespace, to avoid naming collisions and\nto make it look more similar to Java. Objects are allocated on the system heap and\nare deleted after statement compilation is finished. The interface does not use any\nof the Trafodion objects like NAHeap, ComDiagsArea, etc. The interface does use\nC++ STL for strings and collection templates, again with the goal to stay close\nto Java.\n\nNAString ==> std::string\nNAHeap ==> C++ system heap\nComDiagsArea ==> Throw exception with an attached SQLSTATE and error message\n\nIn this first implementation, we only support a C++ interface and the compiler will\ncall that interface directly, without going through the tdm_udrserv process. We may\nneed a special privilege to allow a user to define code that's executed in the\nTrafodion process (the privilege to create a TMUDF that has a compile time interface).\nIn the longer term we hope to support the following flavors:\n\n- C++ and Java interfaces for the TMUDF, both at compile time and run time.\n- Trusted and isolated modes, both at compile and run time.\n\nExample code for a \"sessionize\" TMUDF that expects a single table input and\npasses all columns through to the result, in addition to the session id column\n(assume that session id column is defined as the only output column in the DDL):\n\nclass SessionizeUDFInterface : public TMUDRInterface\n{\n  // override any methods where the UDF author would\n  // like to change the default behavior\n\n  void describeParamsAndColumns(UDRInvocationInfo &info);\n\n};\n\nvoid SessionizeUDFInterface::describeParamsAndColumns(\n     UDRInvocationInfo &info)\n{\n      // sessionize is intended to work with a single input table\n      if (info.getNumTableInputs() != 1)\n        throw UDRException(38001,\n                           \"Expecting one table-valued input, got %d\",\n                           info.getNumTableInputs());\n\n      // add all input table columns as output columns\n      info.addPassThruColumns(0);\n\n}\n\nextern \"C\" TMUDRInterface * SESSIONIZE_CreateCompilerInterfaceObject(\n     const UDRInvocationInfo *info)\n{\n  return new SessionizeUDFInterface();\n}\n",
            "owner_link": "https://api.launchpad.net/devel/~hans-zeller",
            "approver_link": "https://api.launchpad.net/devel/~suresh-subbiah",
            "drafter_link": "https://api.launchpad.net/devel/~hans-zeller",
            "date_created": "2014-10-23T20:02:18.004751+00:00",
            "is_complete": false,
            "implementation_status": "Good progress"
        },
        {
            "starter_link": null,
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-ldap-server/linked_branches",
            "lifecycle_status": "Complete",
            "title": "infra - Add small ldap server on test subnet for auth testing",
            "definition_status": "Obsolete",
            "milestone_link": null,
            "priority": "Medium",
            "http_etag": "\"ad1eff2a8b188f38b05de71d01c971e0f6d3e6a2-0f2dddfd70deac348b74a0629fa16c323a6d45b2\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-ldap-server",
            "information_type": "Public",
            "date_started": null,
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": "https://api.launchpad.net/devel/~steve-varnau",
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-ldap-server/bugs",
            "is_started": false,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-ldap-server/dependencies",
            "specification_url": null,
            "assignee_link": null,
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": false,
            "workitems_text": "",
            "date_completed": "2015-01-13T22:48:42.821798+00:00",
            "name": "infra-ldap-server",
            "whiteboard": null,
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/infra-ldap-server",
            "summary": "Work with security team to set-up ldap server in puppet for authentication and authorization tests.",
            "owner_link": "https://api.launchpad.net/devel/~steve-varnau",
            "approver_link": null,
            "drafter_link": "https://api.launchpad.net/devel/~steve-varnau",
            "date_created": "2014-10-27T19:46:03.357686+00:00",
            "is_complete": true,
            "implementation_status": "Unknown"
        },
        {
            "starter_link": null,
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-restructure-puppet/linked_branches",
            "lifecycle_status": "Not started",
            "title": "infra - Restructure puppet modules",
            "definition_status": "New",
            "milestone_link": null,
            "priority": "Low",
            "http_etag": "\"bbd0b07345f038b9cacf3fcdb1e1ffac06a5629d-32132edc83315675a703f75c53efb07435c0f0b3\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-restructure-puppet",
            "information_type": "Public",
            "date_started": null,
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": null,
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-restructure-puppet/bugs",
            "is_started": false,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-restructure-puppet/dependencies",
            "specification_url": null,
            "assignee_link": null,
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": false,
            "workitems_text": "Work items:\nUpgrade to Puppet3 : INPROGRESS\nSet up a more thorough test environment to test our puppet changes : TODO",
            "date_completed": null,
            "name": "infra-restructure-puppet",
            "whiteboard": "The OpenStack Infrastructure spec for this reorg is located at http://specs.openstack.org/openstack-infra/infra-specs/specs/puppet-modules.html",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/infra-restructure-puppet",
            "summary": "OpenStack Infrastructure team has restructured their infrastructure repository so they can better re-use their puppet modules.  We should consider restructuring our repository so we can take advantage of their changes.",
            "owner_link": "https://api.launchpad.net/devel/~alchen",
            "approver_link": null,
            "drafter_link": "https://api.launchpad.net/devel/~alchen",
            "date_created": "2015-03-04T22:40:15.936515+00:00",
            "is_complete": false,
            "implementation_status": "Unknown"
        },
        {
            "starter_link": null,
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-test-infra/linked_branches",
            "lifecycle_status": "Not started",
            "title": "infra - Need automated test of infra code for slave set-up",
            "definition_status": "Approved",
            "milestone_link": null,
            "priority": "Low",
            "http_etag": "\"c374871e426c8b972c35f52575f6638bd5581dde-aa42b34532c3c5141bd981787a01aa1137808cad\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-test-infra",
            "information_type": "Public",
            "date_started": null,
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": null,
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-test-infra/bugs",
            "is_started": false,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-test-infra/dependencies",
            "specification_url": null,
            "assignee_link": null,
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": false,
            "workitems_text": "",
            "date_completed": null,
            "name": "infra-test-infra",
            "whiteboard": null,
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/infra-test-infra",
            "summary": "Many infra changes infra repo (buildtest module, cloudera module,...) can impact test results, but infra test jobs do not test viability of slave test environment.\n\nWe need dev/test nodes for slaves, and perhaps zuul and jenkins to properly test infra before impacting the production environment. Like install testing, this has restrictions, since puppet runs as root. So this can not be run in unsupervised check testing.\n\nAlso these test machines can not be updated from \"production\" puppet configuration with time-based runs, but must be tested with puppet apply instead.\n\nShould be implemented after infra-upgrade-puppet is implemented.",
            "owner_link": "https://api.launchpad.net/devel/~steve-varnau",
            "approver_link": null,
            "drafter_link": "https://api.launchpad.net/devel/~steve-varnau",
            "date_created": "2014-07-13T23:37:14.695838+00:00",
            "is_complete": false,
            "implementation_status": "Unknown"
        },
        {
            "starter_link": "https://api.launchpad.net/devel/~alchen",
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-setup-mail-server/linked_branches",
            "lifecycle_status": "Started",
            "title": "infra - Set up mail server for trafodion.org",
            "definition_status": "Approved",
            "milestone_link": null,
            "priority": "Low",
            "http_etag": "\"dfd9e89d1b9e2ca9fa33093e61f55df05451fd19-131033490d9850fc3a8ebda9383fed48257ac6d7\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-setup-mail-server",
            "information_type": "Public",
            "date_started": "2014-07-10T02:44:11.058732+00:00",
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": null,
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-setup-mail-server/bugs",
            "is_started": true,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/infra-setup-mail-server/dependencies",
            "specification_url": null,
            "assignee_link": "https://api.launchpad.net/devel/~alchen",
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": false,
            "workitems_text": "Work items:\nAdd DNS & MX record for trafodion.org : DONE\nCreate VM for mail server : DONE\nSet up software on mail server : TODO\nAdd SPF record for trafodion.org : TODO",
            "date_completed": null,
            "name": "infra-setup-mail-server",
            "whiteboard": "Links :\nhttps://github.com/JoshData/mailinabox .  This requires Ubuntu 14.04 which in turn requires our puppetmaster to be a version 3.x\nhttps://www.exratione.com/2012/05/a-mailserver-on-ubuntu-1204-postfix-dovecot-mysql/\nhttps://www.exratione.com/2013/07/installing-roundcube-on-ubuntu-1204/\nhttp://www.spfwizard.net/\n\nDepends on puppet upgrade",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/infra-setup-mail-server",
            "summary": "Set up a mail server for the trafodion.org domain\n\nThis is needed so that trafodion.org addresses resolve properly and can be forwarded to appropriate user email addresses. Automated mail from trafodion domain cannot be sent to launchpad d-list until the from address can be verified as a project member.",
            "owner_link": "https://api.launchpad.net/devel/~alchen",
            "approver_link": null,
            "drafter_link": "https://api.launchpad.net/devel/~alchen",
            "date_created": "2014-07-09T22:39:00.862577+00:00",
            "is_complete": false,
            "implementation_status": "Blocked"
        },
        {
            "starter_link": "https://api.launchpad.net/devel/~hans-zeller",
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/cmp-divisioning/linked_branches",
            "lifecycle_status": "Started",
            "title": "Support for multi-temperature data - part 1",
            "definition_status": "Approved",
            "milestone_link": "https://api.launchpad.net/devel/trafodion/+milestone/r1.0",
            "priority": "Medium",
            "http_etag": "\"91c325c41341683a07d0cf076bb5b75cacee3cfb-01de6e174b76a5e6d47ff09ddac2e46007532c4d\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/cmp-divisioning",
            "information_type": "Public",
            "date_started": "2014-11-19T01:18:53.919266+00:00",
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": null,
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/cmp-divisioning/bugs",
            "is_started": true,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/cmp-divisioning/dependencies",
            "specification_url": "https://wiki.trafodion.org/wiki/index.php/Cmp-divisioning",
            "assignee_link": "https://api.launchpad.net/devel/~hans-zeller",
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": true,
            "workitems_text": "",
            "date_completed": null,
            "name": "cmp-divisioning",
            "whiteboard": "Gerrit topic: https://review.trafodion.org/#q,topic:bug/1388458,n,z\n\nAddressed by: https://review.trafodion.org/741\n    Support for divisioning (multi-temperature data)\n\nThe logical partition approach will be difficult to support both \"DIVISION BY\" and \"PARTITION BY\" on a table, where partition key and division key are different.",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/cmp-divisioning",
            "summary": "Databases that maintain current and historical data often suffer from the problem of decreasing performance as the amount of historical data grows. Most database systems have features that avoid this performance and scalability problem, typically by ensuring that the recent (or “hot”) data is stored separately from the older (“cold”) data. We want to implement multi-temperature data by creating logical, not physical range partitions. We do this by adding the range partition number as a prefix to the key, so that hot data is stored together in a key range and that cold data is stored in a separate key range. We expect that these logical range partitions will exist within each region, and that this is achieved by salting. The range partition number is a computed system column, meaning that it is not normally user-visible and that it is automatically maintained by Trafodion.",
            "owner_link": "https://api.launchpad.net/devel/~hans-zeller",
            "approver_link": "https://api.launchpad.net/devel/~suresh-subbiah",
            "drafter_link": "https://api.launchpad.net/devel/~hans-zeller",
            "date_created": "2014-11-19T00:40:34.749631+00:00",
            "is_complete": false,
            "implementation_status": "Beta Available"
        },
        {
            "starter_link": "https://api.launchpad.net/devel/~ping-lu",
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/connectivity-odbc-compression/linked_branches",
            "lifecycle_status": "Complete",
            "title": "Enable data compression in ODBC driver and DCS server",
            "definition_status": "Approved",
            "milestone_link": "https://api.launchpad.net/devel/trafodion/+milestone/r0.9",
            "priority": "High",
            "http_etag": "\"fdab03a7cdd457603601eb5db3c9968bd87e7c29-d05cf045c3a969655b0707e9377d9a151281c118\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/connectivity-odbc-compression",
            "information_type": "Public",
            "date_started": "2014-08-29T09:08:15.900672+00:00",
            "has_accepted_goal": true,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": "https://api.launchpad.net/devel/~ping-lu",
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/connectivity-odbc-compression/bugs",
            "is_started": true,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/connectivity-odbc-compression/dependencies",
            "specification_url": "https://wiki.trafodion.org/wiki/index.php/Enable_data_compression_in_ODBC_driver_and_DCS_server#Configuration_File_Changes",
            "assignee_link": "https://api.launchpad.net/devel/~ping-lu",
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": true,
            "workitems_text": "",
            "date_completed": "2014-09-19T07:31:09.773594+00:00",
            "name": "connectivity-odbc-compression",
            "whiteboard": "code review\n\n\nGerrit topic: https://review.trafodion.org/#q,topic:bp/connectivity-odbc-compression,n,z\n\n\nAddressed by: https://review.trafodion.org/384\n    ODBC win64 driver compression ability\n",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/connectivity-odbc-compression",
            "summary": "Starting from 0.9, we enabled compression ability for ODBC driver and DCS server. with compression, driver can send big bunch of data quickly and efficiently to DCS server over TCPIP network, and vice versa. with this new feature driver/dcs_server could reduce the sending/receiving data size to 1 of 200-300 of original, depend on data type.",
            "owner_link": "https://api.launchpad.net/devel/~ping-lu",
            "approver_link": null,
            "drafter_link": "https://api.launchpad.net/devel/~ping-lu",
            "date_created": "2014-08-29T09:06:51.461740+00:00",
            "is_complete": true,
            "implementation_status": "Implemented"
        },
        {
            "starter_link": "https://api.launchpad.net/devel/~roberta-marton",
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/security-ansi-schemas/linked_branches",
            "lifecycle_status": "Complete",
            "title": "ANSI Schemas",
            "definition_status": "Approved",
            "milestone_link": "https://api.launchpad.net/devel/trafodion/+milestone/r1.0",
            "priority": "High",
            "http_etag": "\"71ace9684efc1e4dff3d97e43d641a16d483f6ad-2f6a2fe5a01562b3663189fd76ecc441d84a8445\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/security-ansi-schemas",
            "information_type": "Public",
            "date_started": "2014-12-06T00:08:35.388062+00:00",
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": "https://api.launchpad.net/devel/~roberta-marton",
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/security-ansi-schemas/bugs",
            "is_started": true,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/security-ansi-schemas/dependencies",
            "specification_url": "http://docs.trafodion.org/ANSI_Schemas.pdf",
            "assignee_link": "https://api.launchpad.net/devel/~cliff-gray",
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": true,
            "workitems_text": "",
            "date_completed": "2015-01-23T21:36:01.558963+00:00",
            "name": "security-ansi-schemas",
            "whiteboard": null,
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/security-ansi-schemas",
            "summary": "ANSI schemas require a single authorization ID to own and administer a schema.  Trafodion will allow the authorization ID to be a database user or a role.  Creation of objects in a schema will require the schema to have previously been created; no longer will objects be allowed to specify a non-existent schema as a label name qualifier.",
            "owner_link": "https://api.launchpad.net/devel/~cliff-gray",
            "approver_link": null,
            "drafter_link": "https://api.launchpad.net/devel/~cliff-gray",
            "date_created": "2014-11-20T22:50:52.529677+00:00",
            "is_complete": true,
            "implementation_status": "Implemented"
        },
        {
            "starter_link": "https://api.launchpad.net/devel/~vasudev-prashanth",
            "linked_branches_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/dtm-ddl-support/linked_branches",
            "lifecycle_status": "Started",
            "title": "Support for SQL ddl operations within transactions.",
            "definition_status": "Approved",
            "milestone_link": "https://api.launchpad.net/devel/trafodion/+milestone/r2.0",
            "priority": "Undefined",
            "http_etag": "\"487d132b9e747b6301597dc46c3a41bdac23c3c2-f37f4071647f0d996028dd017ddccb47667ac1bb\"",
            "self_link": "https://api.launchpad.net/devel/trafodion/+spec/dtm-ddl-support",
            "information_type": "Public",
            "date_started": "2015-05-01T00:15:18.266144+00:00",
            "has_accepted_goal": false,
            "resource_type_link": "https://api.launchpad.net/devel/#specification",
            "completer_link": null,
            "bugs_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/dtm-ddl-support/bugs",
            "is_started": true,
            "dependencies_collection_link": "https://api.launchpad.net/devel/trafodion/+spec/dtm-ddl-support/dependencies",
            "specification_url": "http://docs.trafodion.org/TransactionalDDL.pdf",
            "assignee_link": "https://api.launchpad.net/devel/~vasudev-prashanth",
            "target_link": "https://api.launchpad.net/devel/trafodion",
            "direction_approved": false,
            "workitems_text": "",
            "date_completed": null,
            "name": "dtm-ddl-support",
            "whiteboard": "Technology Preview available in R1.1",
            "web_link": "https://blueprints.launchpad.net/trafodion/+spec/dtm-ddl-support",
            "summary": "Currently SQL create and drop table is not supported as part of a transaction.  This causes inconsistencies because creates and drops are associated with dml operations against SQL meta-data tables.  ",
            "owner_link": "https://api.launchpad.net/devel/~john-deroo",
            "approver_link": "https://api.launchpad.net/devel/~narendra-goyal",
            "drafter_link": "https://api.launchpad.net/devel/~john-deroo",
            "date_created": "2014-10-07T09:17:00.876284+00:00",
            "is_complete": false,
            "implementation_status": "Beta Available"
        }
    ],
    "resource_type_link": "https://api.launchpad.net/devel/#specification-page-resource"
} 